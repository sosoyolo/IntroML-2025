{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suited-classification",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-compromise",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "This notebook is part of a series of exercises for the CIVIL-226 Introduction to Machine Learning for Engineers course at EPFL. Copyright (c) 2021 [VITA](https://www.epfl.ch/labs/vita/) lab at EPFL  \n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "**Author(s):** [David Mizrahi](mailto:david.mizrahi@epfl.ch)\n",
    "<hr style=\"clear:both\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-madonna",
   "metadata": {},
   "source": [
    "In this exercise, we'll build on what was done in the previous exercise and implement Convolutional Neural Nets with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-drinking",
   "metadata": {},
   "source": [
    "*Run next cell to show tweet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stuck-working",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">A demo from 1993 of 32-year-old Yann LeCun showing off the world&#39;s first convolutional network for text recognition. <a href=\"https://twitter.com/hashtag/tbt?src=hash&amp;ref_src=twsrc%5Etfw\">#tbt</a> <a href=\"https://twitter.com/hashtag/ML?src=hash&amp;ref_src=twsrc%5Etfw\">#ML</a> <a href=\"https://twitter.com/hashtag/neuralnetworks?src=hash&amp;ref_src=twsrc%5Etfw\">#neuralnetworks</a> <a href=\"https://twitter.com/hashtag/CNNs?src=hash&amp;ref_src=twsrc%5Etfw\">#CNNs</a> <a href=\"https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw\">#MachineLearning</a> <a href=\"https://t.co/9eeibjJ4MK\">pic.twitter.com/9eeibjJ4MK</a></p>&mdash; MIT CSAIL #AAAI2021 (@MIT_CSAIL) <a href=\"https://twitter.com/MIT_CSAIL/status/1347237563342340097?ref_src=twsrc%5Etfw\">January 7, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">A demo from 1993 of 32-year-old Yann LeCun showing off the world&#39;s first convolutional network for text recognition. <a href=\"https://twitter.com/hashtag/tbt?src=hash&amp;ref_src=twsrc%5Etfw\">#tbt</a> <a href=\"https://twitter.com/hashtag/ML?src=hash&amp;ref_src=twsrc%5Etfw\">#ML</a> <a href=\"https://twitter.com/hashtag/neuralnetworks?src=hash&amp;ref_src=twsrc%5Etfw\">#neuralnetworks</a> <a href=\"https://twitter.com/hashtag/CNNs?src=hash&amp;ref_src=twsrc%5Etfw\">#CNNs</a> <a href=\"https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw\">#MachineLearning</a> <a href=\"https://t.co/9eeibjJ4MK\">pic.twitter.com/9eeibjJ4MK</a></p>&mdash; MIT CSAIL #AAAI2021 (@MIT_CSAIL) <a href=\"https://twitter.com/MIT_CSAIL/status/1347237563342340097?ref_src=twsrc%5Etfw\">January 7, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-judgment",
   "metadata": {},
   "source": [
    "Before we get started, you'll need to install the [`torchsummary` package](https://github.com/sksq96/pytorch-summary) for a specific section of the exercise.\n",
    "You can do so directly from the command line with:\n",
    "```\n",
    "pip install torchsummary\n",
    "```\n",
    "Make sure that you are in the correct conda environment before running that command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-wrestling",
   "metadata": {},
   "source": [
    "#### For Google Colab\n",
    "You can run this notebook in Google Colab using the following link: https://colab.research.google.com/github/vita-epfl/introML-2021/blob/main/exercises/07-convnets/convnets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "friendly-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "if IN_COLAB:\n",
    "    # Install torchsummary\n",
    "    !pip install torchsummary\n",
    "    # Clone the entire repo to access the files\n",
    "    !git clone -l -s https://github.com/vita-epfl/introML-2021.git cloned-repo\n",
    "    %cd cloned-repo/exercises/07-convnets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-immigration",
   "metadata": {},
   "source": [
    "## 1. Imports & set-up\n",
    "\n",
    "This part is nearly identical to last week's exercise on fully-connected neural networks.\n",
    "\n",
    "More specifically, we define:\n",
    "\n",
    "- the MNIST dataset & dataloader\n",
    "- the training & test loop\n",
    "- a 3-layer fully connected neural net (now called `three_layer_net` instead of `model`)\n",
    "\n",
    "Then this neural net is trained for 10 epochs. This time, we use **Adam instead of SGD** as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupied-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch & torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "\n",
    "# torchsummary\n",
    "import torchsummary\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Helper files\n",
    "import helpers\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continent-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu118'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arabic-durham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1+cu118'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-imagination",
   "metadata": {},
   "source": [
    "As was done last exercise, here is a brief description of these imported packages:\n",
    "\n",
    "**PyTorch:**\n",
    "- `torch.nn` Contains the basic building blocks to implement neural nets (incl. different types of layers and loss functions) | [Documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "- `torch.nn.functional` A functional (stateless) approach to torch.nn, often used for stateless objects (e.g. ReLU) | [Documentation](https://pytorch.org/docs/stable/nn.functional.html) | [More info](https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597/2)\n",
    "- `torch.optim` A package implementing various optimization algorithms, such as SGD and Adam | [Documentation](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "**torchvision:**\n",
    "- `torchvision.transforms` Common image transformations\n",
    "- `torchvision.datasets` Popular image datasets\n",
    "\n",
    "**`torchsummary`:** Provides additional information on network architecture\n",
    "\n",
    "**`tqdm`:** Popular package used to show progress bars | [Documentation](https://tqdm.github.io/)\n",
    "\n",
    "**`helpers`**: Contains functions to help visualize data and predictions\n",
    "\n",
    "**`metrics`:** Contains two simple classes that help keep track and compute the loss and accuracy over a training epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-frontier",
   "metadata": {},
   "source": [
    "### Dataset & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adjusted-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset in a folder called \"data\"\n",
    "root = \"data\"\n",
    "\n",
    "# transforms.ToTensor() is used to convert the downloaded PIL Image to a torch Tensor\n",
    "train_data = MNIST(root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = MNIST(root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "batch_size = 32\n",
    "# Reshuffle training data at every epoch, but not the test data \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pretty-individual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in training data: 60000\n",
      "Images in test data: 10000\n",
      "Mapping from targer value to class name:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '0 - zero',\n",
       " 1: '1 - one',\n",
       " 2: '2 - two',\n",
       " 3: '3 - three',\n",
       " 4: '4 - four',\n",
       " 5: '5 - five',\n",
       " 6: '6 - six',\n",
       " 7: '7 - seven',\n",
       " 8: '8 - eight',\n",
       " 9: '9 - nine'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Images in training data: {len(train_data)}\")\n",
    "print(f\"Images in test data: {len(test_data)}\")\n",
    "# Show the mapping from target value to class name (if you're using MNIST, you won't be too surprised)\n",
    "print(\"Mapping from targer value to class name:\")\n",
    "{i: class_name for i, class_name in enumerate(train_data.classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "short-acoustic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helpers.imshow(torchvision.utils.make_grid(images, nrow=8))\\nprint(targets.reshape(-1, 8))'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "\"\"\"helpers.imshow(torchvision.utils.make_grid(images, nrow=8))\n",
    "print(targets.reshape(-1, 8))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-methodology",
   "metadata": {},
   "source": [
    "### Training loop & test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exterior-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, train_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, epochs: int):\n",
    "    \n",
    "    # Initialize metrics for loss and accuracy\n",
    "    loss_metric = metrics.LossMetric()\n",
    "    acc_metric = metrics.AccuracyMetric(k=1)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        # Progress bar set-up\n",
    "        pbar = tqdm(total=len(train_loader), leave=True)\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        # Iterate through data\n",
    "        for data, target in train_loader:\n",
    "            \n",
    "            # Zero-out the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(out, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics & progress bar\n",
    "            loss_metric.update(loss.item(), data.shape[0])\n",
    "            acc_metric.update(out, target)\n",
    "            pbar.update()\n",
    "            \n",
    "        # End of epoch, show loss and acc\n",
    "        pbar.set_postfix_str(f\"Train loss: {loss_metric.compute():.3f} | Train acc: {acc_metric.compute() * 100:.2f}%\")\n",
    "        loss_metric.reset()\n",
    "        acc_metric.reset()\n",
    "        \n",
    "def test(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    # Initialize accuracy metric\n",
    "    acc_metric = metrics.AccuracyMetric(k=1)\n",
    "    \n",
    "    # Progress bar set-up\n",
    "    pbar = tqdm(total=len(test_loader), leave=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        # Iterate through data\n",
    "        for data, target in dataloader:\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(data)\n",
    "            \n",
    "            # Update accuracy metric\n",
    "            acc_metric.update(out, target)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update()\n",
    "            \n",
    "    # End of epoch, show loss and acc\n",
    "    test_acc = acc_metric.compute() * 100\n",
    "    pbar.set_postfix_str(f\"Acc: {test_acc:.2f}%\")\n",
    "    print(f\"Accuracy is {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-madonna",
   "metadata": {},
   "source": [
    "### Three layer fully-connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "expanded-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet(nn.Module):\n",
    "    \"\"\"3-Layer neural net\"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Flatten to get tensor of shape (batch_size, 784)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts classes by calculating the softmax\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "# Note: Instance is called three_layer_net instead of model this time around\n",
    "three_layer_net = ThreeLayerNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-command",
   "metadata": {},
   "source": [
    "#### Loss & optimizer\n",
    "\n",
    "As before, we'll use the [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss.\n",
    "\n",
    "However, this time, we'll switch up optimizers and use **[Adam](https://pytorch.org/docs/master/generated/torch.optim.Adam.html)** with the default settings for the learning rate and momentum. This should help us get faster convergence than with SGD.\n",
    "\n",
    "Implement both the loss and the optimizer in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Cross-Entropy loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Use Adam with default parameters\n",
    "optimizer = torch.optim.Adam(three_layer_net.parameters())\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-dining",
   "metadata": {},
   "source": [
    "####  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "individual-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975263ab23734b5e8e18ef6a1e11e762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42af92a6f23450fb9f1bcebc227a891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd297cf1ca5443d486abe3285b8b1a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28802ba79e3d47dda1434633c42d7a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a88404708847d785688fd1ef58aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79b13fc3e7748ee943649e42d194233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813d8b5c768549bfa7d3a087a96699d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde591aa4f2d424f855743b089a2b4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c80ea05b26d4d928deadd8880bd6378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0c56b048f4a0da392a3705fe3cb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(three_layer_net, train_loader, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mineral-father",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff6b450234e4b70a437d44f452599e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 97.71%\n"
     ]
    }
   ],
   "source": [
    "test(three_layer_net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-specialist",
   "metadata": {},
   "source": [
    "**Expected result:** >96% test accuracy on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-wales",
   "metadata": {},
   "source": [
    "## 2. LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-novel",
   "metadata": {},
   "source": [
    "In this part, you'll implement a slightly modified version of LeNet5, a convolutional neural network proposed by [Yann Le Cun et al. in 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). LeNet was one of the earliest convolutional neural networks, and helped promote the development of deep learning. Your goal is to reproduce this network architecture from just the paper's figure (see below) and a few extra tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-parker",
   "metadata": {},
   "source": [
    "#### LeNet5\n",
    "\n",
    "<img src=\"images/lenet.png\" width=900></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-vegetation",
   "metadata": {},
   "source": [
    "Here are some tips to help you with your implementation:\n",
    "\n",
    "- Our images are 28x28, but the figure shows 32x32 input images. Can you find a way to make our images fit? **Hint:**  `nn.Conv2d` has a padding parameter.\n",
    "- Both convolutional layers use 5x5 filters with stride 1\n",
    "- Use ReLU as your activation function\n",
    "- Ue Max-Pooling whenever subsampling is needed\n",
    "- You'll need to flatten your tensor at some point\n",
    "- No need to add softmax after the final layer, `nn.CrossEntropyLoss()` adds it automatically\n",
    "\n",
    "Furthermore, here is some helpful documentation:\n",
    "- [`torch.nn` documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "- [`torch.nn.functional` documentation](https://pytorch.org/docs/stable/nn.functional.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accessory-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"LeNet5 from `\"Gradient-Based Learning Applied To Document Recognition\"\n",
    "    <http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf>`_\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ### (Define __init__() and forward())\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.padded = nn.ZeroPad2d(2)\n",
    "        self.conv1 = nn.Conv2d(1,6,5,1,0)\n",
    "        self.sub1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5,1,0)\n",
    "        self.sub2 = nn.MaxPool2d(2,2)\n",
    "        #flatten tensor (2D -> 1D)\n",
    "        self.fct1 = nn.Linear(16*5*5,120)\n",
    "        self.fct2 = nn.Linear(120,84)\n",
    "        self.fct3 = nn.Linear(84,10)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = F.relu(self.padded(x))\n",
    "        x2 = F.relu(self.conv1(x1))\n",
    "        x3 = F.relu(self.sub1(x2))\n",
    "        x4 = F.relu(self.conv2(x3))\n",
    "        x5 = F.relu(self.sub2(x4))\n",
    "        x5 = x5.flatten(1)\n",
    "        x6 = F.relu(self.fct1(x5))\n",
    "        x7 = F.relu(self.fct2(x6))\n",
    "        out = self.fct3(x7)\n",
    "\n",
    "\n",
    "        return out\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts classes by calculating the softmax\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "lenet = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suspended-armstrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "# Check that a forward pass gives the correct output size\n",
    "print(lenet(images).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-purchase",
   "metadata": {},
   "source": [
    "**Expected output:** `torch.Size([32, 10])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "welcome-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Cross-Entropy loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Use Adam with default parameters\n",
    "optimizer = torch.optim.Adam(lenet.parameters())\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-eleven",
   "metadata": {},
   "source": [
    "####  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hawaiian-birmingham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f03e143bb5a4d5db7cf8de995e95924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2e06452b574cf79d9d98048909e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc68c85d383464cac433c5884644d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8c883ab1c0491bac2bc82b47d9d9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1836031db49c40a1a1881260af089ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa8b04540184b819e05d81666c8d575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688811ae41bd4c9380d0c8c78428b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb34103f47341b594e6047be6d8644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75e24368a214c9cb8498651757c6d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368767c891364d3b989596281968de9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lenet, train_loader, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "purple-cancer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be066c7f73d4222b20c7eaedf4caef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 99.03%\n"
     ]
    }
   ],
   "source": [
    "test(lenet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-austria",
   "metadata": {},
   "source": [
    "**Expected result:** >98% test accuracy on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-auditor",
   "metadata": {},
   "source": [
    "#### Visualizing predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-isaac",
   "metadata": {},
   "source": [
    "Let's visualize some of these predictions with the help of `view_prediction()` from `helpers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "turkish-penny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"images, _ = next(iter(test_loader))\\npreds = lenet.predict(images)\\n\\n# Shows the image next to the classifier's softmax score\\n# Show for the first 5 images (change value to see more images)\\nfor i in range(5):\\n    helpers.view_prediction(images[i], preds[i], test_data.classes)\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"images, _ = next(iter(test_loader))\n",
    "preds = lenet.predict(images)\n",
    "\n",
    "# Shows the image next to the classifier's softmax score\n",
    "# Show for the first 5 images (change value to see more images)\n",
    "for i in range(5):\n",
    "    helpers.view_prediction(images[i], preds[i], test_data.classes)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-optimum",
   "metadata": {},
   "source": [
    "## 3. Comparing networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-episode",
   "metadata": {},
   "source": [
    "We've successfully trained two models on the MNIST dataset. But how do they differ? To find out, we'll compare their test accuracy and their architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-mailing",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wrong-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-layer fully-connected net test accuracy:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7a8d2b93ea4f37991c47830d68323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 97.71%\n"
     ]
    }
   ],
   "source": [
    "print(\"3-layer fully-connected net test accuracy:\")\n",
    "test(three_layer_net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "level-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5 test accuracy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84af324da455458f88f8266231a72bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 99.03%\n"
     ]
    }
   ],
   "source": [
    "print(\"LeNet-5 test accuracy\")\n",
    "test(lenet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-survival",
   "metadata": {},
   "source": [
    "#### Model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aware-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 100]          78,500\n",
      "            Linear-2                  [-1, 100]          10,100\n",
      "            Linear-3                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(three_layer_net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "altered-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1            [-1, 1, 32, 32]               0\n",
      "            Conv2d-2            [-1, 6, 28, 28]             156\n",
      "         MaxPool2d-3            [-1, 6, 14, 14]               0\n",
      "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-5             [-1, 16, 5, 5]               0\n",
      "            Linear-6                  [-1, 120]          48,120\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "            Linear-8                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(lenet, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-holly",
   "metadata": {},
   "source": [
    "**Questions:** \n",
    "- Which model has the highest accuracy?\n",
    "- Compare the number of trainable parameters (weights) in both networks? Where do most of LeNet's trainable parameters come from?\n",
    "- Which model takes longer to train? Look at the `it/s` metric displayed next to the progress bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-prior",
   "metadata": {},
   "source": [
    "**Answers:** \n",
    "YOUR ANSWERS HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-lying",
   "metadata": {},
   "source": [
    "## 4. Mixing it up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-patio",
   "metadata": {},
   "source": [
    "LeNet performs quite well on MNIST. But what would happen if we apply a fixed random permutation to the pixels of the images?\n",
    "\n",
    "To find out, we'll create a dataset we'll call permuted MNIST. It simply takes the original dataset, and permutes pixels before feeding images to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-flower",
   "metadata": {},
   "source": [
    "### Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "refined-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed so permutation is identical across runs\n",
    "torch.manual_seed(42)\n",
    "perm_indices = torch.randperm(784)\n",
    "# Set back to random seed\n",
    "torch.random.seed()\n",
    "\n",
    "# The same permutation gets applied to each image \n",
    "permute_transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.flatten()[perm_indices].reshape(1, 28, 28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-castle",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"permuted_train_data = MNIST(root, train=True, transform=permute_transform, download=True)\n",
    "permuted_test_data = MNIST(root, train=False, transform=permute_transform, download=True)\n",
    "\n",
    "batch_size = 32\n",
    "permuted_train_loader = torch.utils.data.DataLoader(permuted_train_data, batch_size=batch_size, shuffle=True)\n",
    "permuted_test_loader = torch.utils.data.DataLoader(permuted_test_data, batch_size=batch_size, shuffle=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Visualize permuted images\n",
    "permuted_images, targets = next(iter(permuted_test_loader))\n",
    "helpers.imshow(torchvision.utils.make_grid(permuted_images, nrow=8))\n",
    "print(targets.reshape(-1, 8))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-crest",
   "metadata": {},
   "source": [
    "Pretty hard for us humans to tell which digit is which, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-latino",
   "metadata": {},
   "source": [
    "**Question:** Before starting the training process, how do you think this random permutation will affect the performance of the two networks (3-layer net and LeNet)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-flavor",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-determination",
   "metadata": {},
   "source": [
    "### Training on permuted images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-military",
   "metadata": {},
   "source": [
    "Let's now train our two network architectures on this permuted dataset. As only the dataset changes, the training procedure will be almost exactly the same as previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-spirit",
   "metadata": {},
   "source": [
    "#### Fully-connected NN (3-layer net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new model based on ThreeLayerNet that will be trained on permuted images\n",
    "permuted_three_layer_net = ThreeLayerNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(permuted_three_layer_net.parameters())\n",
    "\n",
    "train(permuted_three_layer_net, permuted_train_loader, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(permuted_three_layer_net, permuted_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-utility",
   "metadata": {},
   "source": [
    "#### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new model based on LeNet that will be trained on permuted images\n",
    "permuted_lenet = LeNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(permuted_lenet.parameters())\n",
    "\n",
    "train(permuted_lenet, permuted_train_loader, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(permuted_lenet, permuted_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-saver",
   "metadata": {},
   "source": [
    "Our 3 layer net is completely unaffected by the permutation, while the accuracy of LeNet decreases.\n",
    "\n",
    "This is to be expected. A ConvNet makes the explicit assumption that the input are images, which allows it to encode certain properties into the architecture, while a fully-connected neural net makes no assumption of the sort. When these assumptions hold, a ConvNet performs quite well but suffers otherwise. Note that LeNet still performs quite well, in part thanks to the final few fully-connected layers, and because MNIST is a particularly easy dataset, where [most digits can be distinguished pretty well with just one pixel](https://gist.github.com/dgrtwo/aaef94ecc6a60cd50322c0054cc04478). \n",
    "\n",
    "As real-world images don't have all their pixels permuted by a malicious exercise maker, you can safely use ConvNets for most tasks involving images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-finder",
   "metadata": {},
   "source": [
    "Congratulations on finishing this exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-withdrawal",
   "metadata": {},
   "source": [
    "## Additional PyTorch resources\n",
    "- PyTorch basics: https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "- PyTorch cheat sheet: https://pytorch.org/tutorials/beginner/ptcheat.html\n",
    "- Other PyTorch tutorials: https://pytorch.org/tutorials/index.html\n",
    "- PyTorch recipes: https://pytorch.org/tutorials/recipes/recipes_index.html (bite-sized code examples on specific PyTorch features)\n",
    "- PyTorch examples: https://github.com/pytorch/examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
